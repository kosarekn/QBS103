---
title: "20240821_ML_Lecture_Code"
output: pdf_document
date: "2024-08-21"
---
# Introduction to Machine Learning with R

## What is Machine Learning

Machine learning (ML) is a branch of artificial intelligence that seeks to develop alorithms based on input data to generate predictions or classifications on unseen data. Examples of machine learning applications are seen across a variety of fields from finance to healthcare. For example, a stockbroker may apply data scraped from X, TikTok, Instagram, Facebook, or the news to her algorithm to capture social factors impacting trading and adequately predict trends in the stock market. Similarly, a clinical scientist may include histology staining and deleterious genetic markers to classify cancer subtypes. 

It's easy to get excited about implementing ML algorithms in your research given it's popularity, but it is important to take the time to understand the statistical framework behind your models. Today we will be exploring classical machine learning using the R package CARET: Classification and Regression Training. CARET provides functionality to conduct the following tasks: 

* Data Cleaning
* Data Splitting
* Model Fitting
* Data Resampling for Parameter Tuning
* Feature Selection
* Predictive Modeling
* Performance Testing

## Data Cleaning

CARET is powerful, but it does not provide functionality to remove contaminants, conduct log10 transformations, or perform probabilistic quotient normalization. These pre-processing steps, while important, are beyond the scope of this bootcamp, but the QBS program offers classes in ML that can provide in depth instruction on additional data cleaning. 

```{r setup, include=TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                       READ IN LIBRARIES
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
library(caret)
library(ggplot2)
```
CARET includes built in data with which we can begin our foray into machine learning. Today I have chosen a particularly disgusting data set for us to explore. This data set is known as the scat data set. In 2015 the Reid group collected data on animal feces in coastal California. We will be using the data on the animal feces, which includes collection information, size, shape, and consistency of the samples collected, to predict which animal may have produced the feces. 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                     READ IN SCAT DATA
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Let's take a look at what our data look like
head(scat)

# Let's see how many observations and variables are in our example data
dim(scat)

# How many categories are in our classification? 
factor(levels(scat$Species))

# Is there any missing data in our example data? 
length(which(is.na(scat) == TRUE))
```
It looks like our example data contains 110 observations and 19 variables. Our end classifications are bobcat, coyote, and gray_fox. Using our handy `is.na()` function, we determined that we have some rows with missing data that will need cleaning. In the below code block we employ imputation, centering, and scaling to clean our data. After that is complete, I always like to make sure that the imputation worked by, again, checking for missing data.
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                         DATA CLEANING
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# We will use KNN imputation, but there is also the option for tree bagging
preProcValues <- preProcess(scat, method=c("knnImpute","center", "scale" ))
cleanData <- predict (preProcValues, scat)

# Is there any missing data in our example data? 
length(which(is.na(cleanData) == TRUE))
```
## Splitting Data

Looks like our imputation worked! 

Now we are ready to conduct data splitting. We will be splitting our data into a training set and a testing set. Typically, we split the data randomly into 2/3 training and 1/3 testing. 

Note that this can be adjusted depending on our data set. In the case of data sets with a large number of end classifications, it might be wise to conduct stratified splitting, ensuring that the split is done evenly across classes of interest by splitting within each class
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                         DATA SPLITTING
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# We will set a seed so we end up with the same split 
set.seed(3456)

# This code splits our data 
trainIndex <- createDataPartition(cleanData$Species, p = 2/3, list = FALSE, times = 1)

# We assign the 2/3 of data to a training set variable
Train <- cleanData[ trainIndex,]

# We assign the remaining 1/3 of data to a testing set variable
Test <- cleanData[-trainIndex,]

# Let's take a look at the dimensions of our training and testing set to see if that all worked out ok
dim(Train)
dim(Test)
```
## Model Fitting

Our data has now been split into a training and testing set! 

Now comes the fun part, we get to pick a model, and there are so many to choose from. Here is a little run down of a few models you might wish to explore: 

* Random Forest - a random forest model builds a series of decision trees, where each tree is built from a random subset of the data, including variables. A subset of variables are selected at each level of the tree according to which best split the data into groups. 
  - Limitation Alert: random forest models work really well on the data on which they are trained, but are often times inaccurate when they are applied. 

* Support Vector Machine - a support vector machine finds a hyperplane to best divide data defined by support vectors (ie. co-ordninates) in n-dimensional space. In other words, we move low dimensional data into a higher dimensional space by leveraging a kernal function and cross validation to find an optimal transformation. Then we find the best plane that divides our data into classifications. 
  - Advantage Alert: one of the major advantages of SVMs is that they can handle outliers 
* Partial Least Squares Discriminant Analysis (PLS-DA) - PLS-DA is a dimensionality reduction method that incorporates labels in the analysis. PLS-DA uses labels to find the direction of maximum class separation, where principal component analysis (PCA) simply maximizes variance without label information. 

* K-Nearest Neighbor (KNN) - KNN is a non-parametric supervised learning method that uses proximity to make predictions and classifications. 

* Elastic Net - elastic net regression combines the lasso regression penalty with the ridge regression penalty. This eliminates our need to decide whether to select a lasso or ridge regression off the bat because elastic net will shrink the penalty that isn't that consequential. Lasso and ridge regression are beyond the scope of this bootcamp, but you will have extensive instruction on this during your time in QBS!

So which of these models is appropriate for our data set? As a general rule of thumb, you must ALWAYS understand the basic algorithm you are using, its strengths and weaknesses, and what assumptions are inherent. 

CARET has a wrapper of over 200 kinds of models, you can see all of these using `names(getModelInfo())`. 

For our data set, we will employ the random forest model, code for which can be found below: 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                       INITAL MODEL FITTING
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# In this line of code we are indicating that we will be using the default parameters. In our next step this will change because we will be tuning some parameters.
fitControl <- trainControl(method = "none")

# We will set a seed so we end up with the same split 
set.seed(825) 

# Conduct model fitting
Fit1 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl, verbose = FALSE)

# Other Methods: “pls” for PLS-DA, “svmLinear” for linear svm, “svmRadial”, “knn”, “glmnet” for elastic net

Fit1
```
## Cross Validation

Inherent to each model are a collection of parameters which need to be tuned for best model performance. If not re-sampling is done, and parameters are not specified, then default values are used, as we have done in the above code. To properly tune parameters, we use re-sampling procedures. When we do this, we take some portion of the training set to fit a model to, and then test it's performance on the with-held portion of the training set to avoid overfitting. There are several methods for resampling: 

* K-fold Cross Validation - split the data into k approximately even sized pieces, we train on k-1 samples and test on the remaining. This process is repeated k times and enure that the piece remaining changes with each iteration. Common choices for k are 5 or 3. 

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                       K-FOLD CROSS VALIDATION
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# k-fold cross validation
fitControl <- trainControl(method = "cv", number = 5)

# We will set a seed so we end up with the same split 
set.seed(825) 

# Conduct model fitting
Fit2 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl, verbose = FALSE)

Fit2
```

* Repeated K-fold Cross Validation - k-fold cross validation, but you repeat the process R times where each time you split the data in a different spot. This method of cross validation is robust to "bad" cuts of the data. 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                 REPEATED K-FOLD CROSS VALIDATION
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# repeated k-fold cross validation
fitControl <- trainControl(method = "repeatedcv", number = 5,repeats = 10)

# We will set a seed so we end up with the same split 
set.seed(825) 

# Conduct model fitting
Fit3 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl, verbose = FALSE)

Fit3
```
* Leave-One-Out-Cross Validation - like k-fold cross validation, except you leave only 1 sample out for testing each time. This is consequentially the same a k-fold cross validation if k=n. A major limitation of this method is overfitting, which is why this method is seldom used save for very small data sets to provide some degree of generalizability. 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                 LEAVE-ONE-OUT CROSS VALIDATION
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Leave one out cross validation
fitControl <- trainControl(method = "LOOCV")


# We will set a seed so we end up with the same split 
set.seed(825) 

# Conduct model fitting
Fit4 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl, verbose = FALSE)

Fit4
```
* Bootstrapping - re-sample from the training set, with replacement, until you hit a training set of the same size as your first set. Build a model on this new re-sampled training set, test on any samples not included in the original set. Repeat this many times. 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                         BOOTSTRAPPING
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Leave one out cross validation
fitControl <- trainControl(method = "boot", number = 100)

# We will set a seed so we end up with the same split 
set.seed(825) 

# Conduct model fitting
Fit5 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl, verbose = FALSE)

Fit5
```
* Grid Search - tries out all the possible combinations of hyperparameters in a predefined grid. The default number of parameters tested over a grid is 3, but this can be easily increased to test more parameter values.
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                         GRID SEARCH
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Note that this is the same as the bootstrapping!
fitControl <- trainControl(method = "boot", number = 100)

# We will set a seed so we end up with the same split 
set.seed(825) 

# NOTE: tuneLength = 5 here, this is the grid search! 
Fit6 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl, tuneLength = 5, verbose = FALSE)

Fit6
```
* Class Imbalance - our data would be considered unbalanced if there is many more samples in one class compared to another. As mentioned earlier it is good to know the strengths and weaknesses of our models. Some algorithms are sensitive to class imbalance, including random forest and k-nearest neighbor. This can sometimes be improved using up or down sampling. 

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                         CLASS IMBALANCE
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Note that this is the same as the bootstrapping, but here we add downsampling!
fitControl <- trainControl(method = "boot", number = 100 , sampling = "down")

# We will set a seed so we end up with the same split 
set.seed(825) 

# NOTE: tuneLength = 5 here
Fit7 <- train(Species ~ ., data = Train, method = "rf", trControl = fitControl,
tuneLength = 5, verbose = FALSE)

Fit7
```
## Feature Selection

Some algorithms, like random forest, have a built in way of assessing variable importance, but many models do not. For these models, CARET uses a filter approach where ROC curve analysis is conducted and the area under the curve for each individual variable is used as a measure of importance.
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                       FEATURE SELECTION
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Select the most important variables
Imp <- varImp(Fit1, scale = FALSE)

# Make a plot of which variables are the most important
plot(Imp, top = 20)

# Select the top 15 most important variables
featSel<-which(Imp$importance$Overall>=15)
length(featSel)

# Subset the training set to contain only those variables
featDF<-Train[,featSel]
```

Recursive Feature Elimination (RFE) leverages nested cross validation to recursively eliminate features to some optimal number. All features are ranked by importance, and then unimportance. Features are removed to achieve optimal model performace. Note that this can take a very long time! 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#                       FEATURE SELECTION
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This function generates a control object that can be used to specify the details of the feature selection algorithms used in this package
rctrl1 <- rfeControl(method = "cv", functions = caretFuncs, allowParallel = F)

trainCtl1<-trainControl(method = "cv", number=3, classProbs = TRUE, allowParallel = F)

# We will set a seed so we end up with the same split 
set.seed(24)

# Run simple backward selection 
Fit8 <- rfe(x=Train,y=Train$Species,
             sizes = c(5,10,15,20,25,30,35,40,45,50,75,100),
             method = "rf", trControl = trainCtl1, tuneLength = 10, rfeControl = rctrl1)

Fit8
```
## Predictive Modeling and Validation

Now since, we used random forest, we won't need to add feature selection, so let's go ahead and use our model with repeated k-fold cross validation (model name: Fit3) on our testing data set. 
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#               PREDICTIVE MODELING AND VALIDATION
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
preds<-predict(Fit3,Test)
confusionMatrix(preds,Test$Species,positive="coyote")
```

